{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d63afa",
   "metadata": {},
   "source": [
    "# Billion Dollar Meteorological Disasters in the United States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1311d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Business Context.** While natural *events* often cannot be avoided, the risks they present can be managed, either by mitigation, avoidance, or insurance, in order to prevent them from becoming natural *disasters*. The consultancy firm you work for has been hired by an independent advocacy group that wants to conduct an analysis of the US emergency management system of preparedness, protection, mitigation, response, and recovery, with the purpose of proposing legislative reforms to make it more effective and financially efficient. Their ultimate goal is to help increase the government's ability to prevent disasters from happening and reduce the negative impact of those that cannot be completely avoided.\n",
    "\n",
    "**Business Problem.** The client would like to know which storm event types are more likely to become disasters, and in which locations, as measured by the number of deaths, injuries, and economic damage they cause. Additionally, they would like to conduct a preliminary assessment of whether the [Post-Katrina Emergency Management Reform Act of 2006](https://www.congress.gov/bill/109th-congress/senate-bill/3721) had any impact on the severity of the disasters that occurred after the bill was signed. This Act centralized the US emergency management under the coordination of the Federal Emergency Management Agency (FEMA) as a response to the enormous human and material losses that were caused by Hurricane Katrina in August 2005.\n",
    "\n",
    "**Analytical Context.** The dataset is a compressed GZIP file of storm events from 1970 to 2020 as recorded by the US [National Oceanic and Atmospheric Administration](https://www.ncdc.noaa.gov/stormevents/ftp.jsp). You can check the [documentation](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import pingouin as pg\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5756023",
   "metadata": {},
   "source": [
    "## Loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd70ce41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>BEGIN_YEAR</th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>TOTAL_DEATHS</th>\n",
       "      <th>TOTAL_INJURIES</th>\n",
       "      <th>TOTAL_DAMAGE_DEFLATED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9987739</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>hail</td>\n",
       "      <td>1983-07-22 16:40:00</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>CST</td>\n",
       "      <td>1983-07-22 16:40:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.72</td>\n",
       "      <td>-104.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9987740</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>hail</td>\n",
       "      <td>1983-07-22 16:45:00</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>CST</td>\n",
       "      <td>1983-07-22 16:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.73</td>\n",
       "      <td>-104.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9987741</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>hail</td>\n",
       "      <td>1983-07-22 16:45:00</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>CST</td>\n",
       "      <td>1983-07-22 16:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.73</td>\n",
       "      <td>-104.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9987735</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>hail</td>\n",
       "      <td>1983-07-22 16:20:00</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>CST</td>\n",
       "      <td>1983-07-22 16:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.73</td>\n",
       "      <td>-104.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9987736</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>hail</td>\n",
       "      <td>1983-07-22 16:25:00</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>CST</td>\n",
       "      <td>1983-07-22 16:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.72</td>\n",
       "      <td>-104.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EPISODE_ID  EVENT_ID     STATE EVENT_TYPE     BEGIN_DATE_TIME  BEGIN_YEAR  \\\n",
       "0         NaN   9987739  COLORADO       hail 1983-07-22 16:40:00      1983.0   \n",
       "1         NaN   9987740  COLORADO       hail 1983-07-22 16:45:00      1983.0   \n",
       "2         NaN   9987741  COLORADO       hail 1983-07-22 16:45:00      1983.0   \n",
       "3         NaN   9987735  COLORADO       hail 1983-07-22 16:20:00      1983.0   \n",
       "4         NaN   9987736  COLORADO       hail 1983-07-22 16:25:00      1983.0   \n",
       "\n",
       "  CZ_TIMEZONE       END_DATE_TIME TOR_F_SCALE BEGIN_LOCATION END_LOCATION  \\\n",
       "0         CST 1983-07-22 16:40:00         NaN            NaN          NaN   \n",
       "1         CST 1983-07-22 16:45:00         NaN            NaN          NaN   \n",
       "2         CST 1983-07-22 16:45:00         NaN            NaN          NaN   \n",
       "3         CST 1983-07-22 16:20:00         NaN            NaN          NaN   \n",
       "4         CST 1983-07-22 16:25:00         NaN            NaN          NaN   \n",
       "\n",
       "   BEGIN_LAT  BEGIN_LON  END_LAT  END_LON  TOTAL_DEATHS  TOTAL_INJURIES  \\\n",
       "0      39.72    -104.60      NaN      NaN             0               0   \n",
       "1      39.73    -104.87      NaN      NaN             0               0   \n",
       "2      39.73    -104.93      NaN      NaN             0               0   \n",
       "3      39.73    -104.85      NaN      NaN             0               0   \n",
       "4      39.72    -104.82      NaN      NaN             0               0   \n",
       "\n",
       "   TOTAL_DAMAGE_DEFLATED  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the `parse_dates` argument to tell pandas which columns should be interpreted as dates.\n",
    "df = pd.read_csv(\"data/dataset.csv.gz\", parse_dates=[\"BEGIN_DATE_TIME\", \"END_DATE_TIME\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b158102",
   "metadata": {},
   "source": [
    "Here is a description of the imported columns:\n",
    "\n",
    "1. **EPISODE_ID**: The storm episode ID. A single episode can contain multiple events\n",
    "2. **EVENT_ID**: This is the ID of the actual storm as such. Several storms can be grouped into an episode\n",
    "3. **STATE**: The state or region where the event occurred\n",
    "4. **EVENT_TYPE**: The type of the event\n",
    "5. **BEGIN_DATE_TIME**: The date and time when the event started. Times and dates are in LST (Local Solar Time), which means that they reflect the local time, not a coordinated time\n",
    "6. **BEGIN_YEAR**: The year in which the event begun\n",
    "7. **CZ_TIMEZONE**: The timezone of the place where the event occurred\n",
    "8. **END_DATE_TIME**: The date and time when the event ended. Times and dates are in LST (Local Solar Time), which means that they reflect the local time, not a coordinated time\n",
    "9. **TOR_F_SCALE**: The [enhanced Fujita scale](https://en.wikipedia.org/wiki/Enhanced_Fujita_scale) (highest recorded value). This scale measures the strength of a tornado based on the amount of damage that it caused. A level of `EF0` means \"light damage\" (wind speeds of 40 - 72 mph), and a level of `EF5` means \"incredible damage\" (261 - 318 mph). `EFU` means \"Unknown\"\n",
    "10. **BEGIN_LOCATION**: The name of the city or village where the event started\n",
    "11. **END_LOCATION**: The name of the city or village where the event ended\n",
    "12. **BEGIN_LAT**: The latitude of the place where the event begun\n",
    "13. **BEGIN_LON**: The longitude of the place where the event begun\n",
    "14. **END_LAT**: The latitude of the place where the event ended\n",
    "15. **END_LON**: The longitude of the place where the event ended\n",
    "16. **TOTAL_DEATHS**: Deaths directly or indirectly attributable to the event\n",
    "17. **TOTAL_INJURIES**: Injuries directly or indirectly attributable to the event\n",
    "18.  **TOTAL_DAMAGE_DEFLATED**: Estimated damage to property and crops in dollars. These dollars are \"real\" dollars, which means that the damages for all the years have been converted ([deflated](https://faculty.fuqua.duke.edu/~rnau/Decision411_2007/411infla.htm)) to the value they would have had in 1982-84. This was done to make the damages comparable across years, since dollars [change purchasing power every year](https://www.insider.com/fast-food-burgers-cost-every-year-2018-9) due to inflation. The deflation was done using the Bureau of Labor Statistics Urban Consumer Price Index, whose base period is 1982-84."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab9965",
   "metadata": {},
   "source": [
    "There are 62 unique storm types, and 2,483,191 occurrences from 1970 to 2020 in the dataset analyzed. In a random sample of 20,000 storms from 1970 to 2020, roughly 49% of the storms recorded begin and end in the same location with 21 unique types of 68. These include thunderstorm wind, hail, floods, tornados, marine thunderstorm wind, flash floods, waterspouts, lightning, marine hail, funnel clouds, heavy rain, dust devil, marine strong wind, debris flow, and marine high wind among the most frequent. <br><br>Additionally, a rough estimate of 49% of storms does not end in the same location, about ~46 unique types of 68. It is interesting to note the number of unique storm types that do not end in the same location. However, whether the storms begin and end in the same location, thunderstorm-wind storm type has the highest frequency. We can also note that Texas has the highest number of storm occurrences. <br><br>\n",
    "Overall, in the geo scatter map in 5.1, we can see that the Plains, Northeastern, Central, South, and Southeast regions typically experience a higher frequency of disasters.<br><br>\n",
    "Note: I processed the analysis on a smaller random sample to speed up processing and prevent the kernel from consistently dying. Consequently, the visualization in 5.1 may not depict the precise number of data points on the map.<br><br>\n",
    "A choropleth map may be another visualization tool when considering how to effectively find patterns related to the size of risk area around storm events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7806a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data (this df is used in various exercise cells below)\n",
    "\n",
    "#Creating a separate df for mapping \n",
    "begin_loc_map = pd.DataFrame(df)#.dropna(subset=['BEGIN_LAT', 'BEGIN_LON',]) # 'BEGIN_LOCATION', 'END_LOCATION'\n",
    "\n",
    "#creating a smaller random sample to speed up processing \n",
    "begin_loc_map_sample = begin_loc_map.sample(n=20000)\n",
    "\n",
    "#subsetting only necessary columns for better readability\n",
    "begin_loc_map_sample = begin_loc_map_sample[['EVENT_ID', 'STATE', 'EVENT_TYPE', 'BEGIN_DATE_TIME',\n",
    "       'BEGIN_YEAR', 'BEGIN_LOCATION', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT',\n",
    "       'END_LON', 'TOTAL_DAMAGE_DEFLATED']]\n",
    "\n",
    "#converting YEAR to Int to add to map hover text below\n",
    "begin_loc_map_sample['year'] = begin_loc_map_sample['BEGIN_DATE_TIME'].dt.year.astype('Int64')\n",
    "\n",
    "#bool to identify which events begin and end location coincide\n",
    "begin_loc_map_sample['event_location_t_f'] = begin_loc_map_sample['BEGIN_LOCATION'] == begin_loc_map_sample['END_LOCATION']\n",
    "\n",
    "#concactinating info columns to add to map hover text below\n",
    "begin_loc_map_sample['text'] = begin_loc_map_sample['STATE'] + '<br>begin location: ' + begin_loc_map_sample['BEGIN_LOCATION'] + '<br>end location: ' + begin_loc_map_sample['END_LOCATION'] +'<br>type: ' + begin_loc_map_sample['EVENT_TYPE'] + '<br>' + begin_loc_map_sample['year'].astype(str)\n",
    "\n",
    "#sample df of storm events which locations coincide to create Plotly trace1\n",
    "sample_coincide = begin_loc_map_sample.copy()\n",
    "sample_coincide = sample_coincide.loc[sample_coincide['event_location_t_f'] == True]\n",
    "\n",
    "#sample df of storm events which locations DO NOT coincide to create Plotly trace2\n",
    "sample_DN_coincide = begin_loc_map_sample.copy()\n",
    "sample_DN_coincide = sample_DN_coincide.loc[sample_DN_coincide['event_location_t_f'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly scatter mapbox map of Storm Events whose BEGIN_LOCATION and END_LOCATION do not coincide \n",
    "#vs. those in which they do coincide. \n",
    "\n",
    "#generating Plotly Scatter Mapbox\n",
    "fig = px.scatter_mapbox( \n",
    "                        lat=['37.09'], \n",
    "                        lon=['-95.71'], \n",
    "                        zoom=2.6, \n",
    "                        height=650,\n",
    "                        mapbox_style='carto-darkmatter',\n",
    "                        title = '<b>Storm locations from 1970 to 2020 <br> Sample size: 20,000</b> <br> (Toggle legend to show separate groups)'\n",
    "                       )\n",
    "\n",
    "#Trace1 layer on map from 'sample_coincide' df of storm events whose begin and end location coincide\n",
    "fig.add_trace(go.Scattermapbox(lon=sample_coincide['BEGIN_LON'],\n",
    "                               lat=sample_coincide['END_LAT'],\n",
    "                               name='Coincide',\n",
    "                               hovertemplate = sample_coincide['text'],\n",
    "                               opacity= .4,\n",
    "                               mode='markers',\n",
    "                               marker=dict(\n",
    "                                       size= 5,\n",
    "                                       color = 'magenta',\n",
    "                                       opacity = .8,),\n",
    "                                    ))\n",
    "\n",
    "#Trace2 layer on map from 'sample_DN_coincide' df of storm events whose begin and end location do not coincide\n",
    "fig.add_trace(go.Scattermapbox(lon=sample_DN_coincide['BEGIN_LON'],\n",
    "                               lat=sample_DN_coincide['END_LAT'],\n",
    "                               name='Do Not Coincide',\n",
    "                               hovertemplate = sample_DN_coincide['text'],\n",
    "                               mode='markers',\n",
    "                               marker=dict(\n",
    "                                       size= 5,\n",
    "                                       color = '#DFFF00',\n",
    "                                       opacity = 1),\n",
    "                                    ))\n",
    "\n",
    "#defining legend\n",
    "fig.update_layout(legend = dict(bordercolor='black',\n",
    "                                borderwidth=22,\n",
    "                                itemclick= 'toggleothers',\n",
    "                                bgcolor='#000003',\n",
    "                                font_size=12,\n",
    "                                x=0.9,\n",
    "                                y=0.9,\n",
    "                                traceorder='normal',\n",
    "                                font=dict(family='monospace',\n",
    "                                          size=12,\n",
    "                                          color='white',\n",
    "                                         )\n",
    "                               ),\n",
    "                 legend_title='<b>Begin & End<br>Locations:</b><br>'\n",
    "                 )\n",
    "\n",
    "#updating title and map background parameters\n",
    "fig.update_layout(title_x=0.45,\n",
    "                  title_y=0.95,\n",
    "                  font_color='white',\n",
    "                  title_font_size = 18,\n",
    "                  title_font_family='monospace',\n",
    "                  #mapbox={'style': 'carto-darkmatter', 'center': {'lon':-95.71, 'lat' : 37.09}, 'zoom': 3}, # (optoin 2)\n",
    "                  margin={'r':0,'t':0,'l':0,'b':0},\n",
    "                 )\n",
    "\n",
    "#updating margin and hoverlabels\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},  # remove the white gutter between the frame and map\n",
    "                    # hover appearance\n",
    "                    hoverlabel=dict(bgcolor='white',     # white background\n",
    "                                    font_size=12,        # label font size\n",
    "                                    font_family='monospace') # label font\n",
    "                 )\n",
    "\n",
    "#updating map background parameters\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"white-bg\",\n",
    "    mapbox_layers=[\n",
    "        {\n",
    "            \"below\": 'traces',\n",
    "            \"sourcetype\": \"raster\",\n",
    "            \"sourceattribution\": \"United States Geological Survey\",\n",
    "            \"source\": [\n",
    "                \"https://basemap.nationalmap.gov/arcgis/rest/services/USGSImageryOnly/MapServer/tile/{z}/{y}/{x}\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"sourcetype\": \"raster\",\n",
    "            \"sourceattribution\": \"Government of Canada\",\n",
    "            \"source\": [\"https://geo.weather.gc.ca/geomet/?\"\n",
    "                       \"SERVICE=WMS&VERSION=1.3.0&REQUEST=GetMap&BBOX={bbox-epsg-3857}&CRS=EPSG:3857\"\n",
    "                       \"&WIDTH=1000&HEIGHT=1000&LAYERS=RADAR_1KM_RDBR&TILED=true&FORMAT=image/png\"],\n",
    "        }\n",
    "      ])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7df1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical summary for categorical variables on sample set\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "table_cat = ff.create_table(begin_loc_map_sample.describe(include=['O']).T, index=True, index_title='Categorical columns')\n",
    "table_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_coincide.EVENT_TYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_DN_coincide.EVENT_TYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5179c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_coincide[['EVENT_TYPE']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_DN_coincide.EVENT_TYPE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d022385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Frequency table of event types on entire df\n",
    "# df.EVENT_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80382260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency table of event types on sample set\n",
    "begin_loc_map_sample.EVENT_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Frequency table of BEGIN_LOCATION on entire df\n",
    "# df.BEGIN_LOCATION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency table of BEGIN_LOCATION on sample set\n",
    "begin_loc_map_sample.BEGIN_LOCATION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaaca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contingency table to validate the number of storms per month by type\n",
    "crosstab = pd.crosstab(index=begin_loc_map_sample['EVENT_TYPE'], columns=begin_loc_map_sample['BEGIN_LOCATION'])\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a884c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2 = chi2_contingency(crosstab)\n",
    "chi2\n",
    "print('The P-value for EVENT_TYPE and BEGIN_LOCATION on sample set is: ', chi2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea35d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_loc_map_sample.groupby('EVENT_TYPE')['BEGIN_LOCATION'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616454a",
   "metadata": {},
   "source": [
    "## Data Visualizations of Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665205f",
   "metadata": {},
   "source": [
    "Below are data visualization strategis that includes metrics and plots of different kinds to estimate risk assessment:\n",
    "\n",
    "1. A time series barplot by month and storm type is an effective option for visualizing which storm types are most likely to happen in a given month. We can also use a heatmap to visualize the relationship between storm type per month as well as a frequency table.\n",
    "<br>\n",
    "<br>\n",
    "2. Because of the number of unique storm types, a density heatmap can be an effective tool to visualize the economic damange caused by storms. \n",
    "<br>\n",
    "<br>\n",
    "3. A choropleth map can be useful for visualizing which locations the storms are most likely to happen by plotting the count per geo location identifier. A density heatmap is another tool we can use to visualize this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4713a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating df for contingency table and multiple plots below\n",
    "\n",
    "month_event_df = df[['STATE', 'EVENT_TYPE', 'TOTAL_DAMAGE_DEFLATED', 'BEGIN_DATE_TIME','BEGIN_LOCATION', 'END_LOCATION']]#.dropna()\n",
    "\n",
    "#Creating df copy to slice df accordingly. \n",
    "month_event_df = month_event_df.copy()\n",
    "\n",
    "#Accessing month from BEGIN_DATE_TIME column as int64 for readibility\n",
    "month_event_df['event_month'] = month_event_df['BEGIN_DATE_TIME'].dt.month.astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f3f39",
   "metadata": {},
   "source": [
    "### No. 1 - Storm types likely to happen in a given month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713631a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly density heatmap of storm types likely to happen in a given month\n",
    "\n",
    "month_event_df_drop = month_event_df.dropna(subset=['event_month'])\n",
    "\n",
    "fig = px.density_heatmap(month_event_df_drop,\n",
    "                         x='event_month',\n",
    "                         y='EVENT_TYPE',\n",
    "                         color_continuous_scale='Viridis',\n",
    "#                          nbinsx=12,\n",
    "#                          nbinsy=14,\n",
    "                         labels={col:col.replace('_', ' ') for col in month_event_df_drop.columns},\n",
    "                         title='<b>Density heatmap of storms most likely to happen by month </b><br>1970 - 2020',\n",
    "                         )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contingency table to validate the number of storms per month by type\n",
    "\n",
    "month_event_cont_table = pd.crosstab(index=month_event_df['EVENT_TYPE'], columns=month_event_df['event_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c34dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn density heatmap of storm types likely to happen in a given month\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "ax1 = sns.heatmap(month_event_cont_table, cmap=\"Blues\")\n",
    "ax1.set_title('Density Heatmap of Event Type vs. Month');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b549a43",
   "metadata": {},
   "source": [
    "### No. 2 - How large the economic damages caused by the storms would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a separate df for economic damages deflated\n",
    "\n",
    "total_loss_map = df.dropna(subset=['BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'TOTAL_DAMAGE_DEFLATED'])\n",
    "\n",
    "#creating a smaller random sample to speed up processing \n",
    "total_loss_map_sample = total_loss_map.sample(n=20000)\n",
    "\n",
    "#subsetting only necessary columns for better readability\n",
    "total_loss_map_sample = total_loss_map_sample[['STATE', 'EVENT_TYPE', 'BEGIN_DATE_TIME',\n",
    "       'BEGIN_YEAR', 'BEGIN_LOCATION', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT',\n",
    "       'END_LON', 'TOTAL_DAMAGE_DEFLATED']]\n",
    "\n",
    "#Converting total_damages_deflated from float to string to concat with hover text column only\n",
    "total_loss_map_sample['total_damages_deflated'] = [\"$%.4f\" % i for i in total_loss_map_sample['TOTAL_DAMAGE_DEFLATED']]\n",
    "\n",
    "total_loss_map_sample['year'] = [\"%.f\" % i for i in total_loss_map_sample['BEGIN_YEAR']]\n",
    "\n",
    "#converting YEAR to Int to add to hover text column below\n",
    "total_loss_map_sample['year_int'] = total_loss_map_sample['BEGIN_DATE_TIME'].dt.year.astype('Int64')\n",
    "\n",
    "#converting TOTAL_DAMAGE_DEFLATED column from float to string to add to hover text column below\n",
    "total_loss_map_sample['total_damages_deflated_f'] = [\"%.f\" % i for i in total_loss_map_sample['TOTAL_DAMAGE_DEFLATED']]\n",
    "\n",
    "#concactinating info columns for hover text over map\n",
    "total_loss_map_sample['text'] = total_loss_map_sample['STATE'] + '<br>type: ' + total_loss_map_sample['EVENT_TYPE'] + '<br>economic damages: ' + total_loss_map_sample['total_damages_deflated'] + '<br>' + total_loss_map_sample['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zipping lat, lon columns - preparing dataset for folium map BELOW\n",
    "\n",
    "total_loss_map_sample_zip = list(zip(total_loss_map_sample['BEGIN_LAT'], total_loss_map_sample['BEGIN_LON'], total_loss_map_sample['TOTAL_DAMAGE_DEFLATED']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ce919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium  Heatmap weighted based on the total damage of events per location.\n",
    "initial_coords = [44.08, -103.23]\n",
    "folium_loss_hmap = folium.Map(location=initial_coords, zoom_start=2.5, tiles='CartoDB dark_matter')\n",
    "\n",
    "#folium.Marker([25.76, -80.19], popup='Hello from Miami!').add_to(folium_loss_hmap)\n",
    "\n",
    "\n",
    "hm_layer = HeatMap(total_loss_map_sample_zip,\n",
    "                   #Parameters to adjust tiles color, size, blur\n",
    "                   min_opacity=0.45,\n",
    "                   radius=4.5,\n",
    "                   blur=3.75, \n",
    "                 )\n",
    "folium_loss_hmap.add_child(hm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b55e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly go.Scattergeo (bubble map) of economic damages caused by the storms \n",
    "\n",
    "df2 = total_loss_map_sample\n",
    "df2['text'] = df2['BEGIN_LOCATION'] + '<br>type: ' + df2['EVENT_TYPE'] + '<br>econmonic damage: ' + '<br>' + df2['year'] + (df2['TOTAL_DAMAGE_DEFLATED']/1e6).astype(str) +' Million'\n",
    "limits = [(0,1),(2,3),(4,10),(11,20),(21,50),(51,300),(301,1000),(1001,5000),(5001,10000)]\n",
    "colors = ['royalblue','crimson','lightseagreen','orange','chartreuse','cadetblue', 'royalblue','indigo','limegreen']\n",
    "scale = 100000\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(limits)):\n",
    "    lim = limits[i]\n",
    "    df_sub = df2[lim[0]:lim[1]]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode = 'ISO-3',\n",
    "        lon = df_sub['BEGIN_LON'],\n",
    "        lat = df_sub['BEGIN_LAT'],\n",
    "        hovertext = df2['text'],\n",
    "        marker = dict(\n",
    "            size = df_sub['TOTAL_DAMAGE_DEFLATED']/scale,\n",
    "            color = colors[i],\n",
    "            line_color='rgb(40,40,40)',\n",
    "            line_width=0.5,\n",
    "            sizemode = 'area'\n",
    "        ),\n",
    "        name = '{0} - {1}'.format(lim[0],lim[1])\n",
    "    ))\n",
    "                  \n",
    "fig.update_layout(\n",
    "        title_text = '<b>Economic damages caused by storms from 1970 to 2020</b><br>Sample size: 20,000<br>(Hover over map to view event info)',\n",
    "        showlegend = False,\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            landcolor = 'rgb(217, 217, 217)',\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2843200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly bar plot of economic damages by storm type\n",
    "\n",
    "fig = px.bar(total_loss_map_sample, \n",
    "             x='EVENT_TYPE', \n",
    "             y='TOTAL_DAMAGE_DEFLATED', \n",
    "             color='EVENT_TYPE', \n",
    "             title='<b>Economic damages by storm type from 1970-2020 | Sample size: 20,000<br></b>(De-select event types to view storms separately. Hover over for event info)',\n",
    "             hover_data=['STATE', 'EVENT_TYPE', 'TOTAL_DAMAGE_DEFLATED','year'],\n",
    "             labels={col:col.replace('_', ' ') for col in total_loss_map_sample.columns}) # remove underscore\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly bar plot of economic damages by storm type\n",
    "\n",
    "total_damages_by_event = total_loss_map_sample.groupby('EVENT_TYPE',)['TOTAL_DAMAGE_DEFLATED'].sum().reset_index()\n",
    "fig = px.bar(total_damages_by_event, \n",
    "             x='EVENT_TYPE', \n",
    "             y='TOTAL_DAMAGE_DEFLATED', \n",
    "             color='EVENT_TYPE', \n",
    "             title='<b>Economic damages by storm type from 1970-2020 | Sample size: 20,000<br></b>(Deselect event types in legend to view storms separately. Hover over for event info)',\n",
    "             labels={col:col.replace('_', ' ') for col in total_damages_by_event.columns}) # remove underscore\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb2c12",
   "metadata": {},
   "source": [
    "### No. 3 - Locations storms are most likely to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6033538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contingency table to validate the number of storms per month by type\n",
    "event_type_cont_table = pd.crosstab(index=month_event_df['EVENT_TYPE'], columns=month_event_df['STATE'])#.apply(np.log)\n",
    "event_type_cont_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b135891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn density heatmap of locations (by STATE) storms are most likely to happen\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "ax2 = sns.heatmap(event_type_cont_table, cmap=\"Blues\");\n",
    "ax2.set_title('Event Type vs. Location');\n",
    "ax2.set_title('Density Heatmap of Event Type vs. Locations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa787293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly density heatmap of locations (by STATE) storms are most likely to happen\n",
    "\n",
    "fig = px.density_heatmap(month_event_df,\n",
    "                         x='STATE',\n",
    "                         y='EVENT_TYPE',\n",
    "                         color_continuous_scale='icefire',\n",
    "#                          nbinsx=60,\n",
    "#                          nbinsy=60,\n",
    "                         )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9736d8",
   "metadata": {},
   "source": [
    "### Hypothesis Testing with Pingouin Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf933d3",
   "metadata": {},
   "source": [
    "Conducting a hypothesis test for each event type to assess whether there is a difference in average total damage when comparing disasters that happened before the reform to those that happened after and keeping only the event types that result in a significant difference (using a significance threshold of $\\alpha=0.01$). Since it is likely that not all events that have happened in the US are present in this dataset, we can interpret the data as being a sample (conducting hypothesis tests on population data would not make sense).\n",
    "\n",
    "**Note:** The events that do not have associated events either before or after the the Post-Katrina Emergency Management Act of 2006 are ignored (since a $t$ - test won't be possible). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"POST_ACT\"] = df[\"BEGIN_YEAR\"] > 2006\n",
    "\n",
    "def test_differences(df):\n",
    "    \"\"\"\n",
    "    Conducts a t-test on TOTAL_DAMAGES comparing events\n",
    "    that happened in 2006 or before with events that\n",
    "    happenned after that year.\n",
    "    \n",
    "    Inputs:\n",
    "    `df`: A pandas DataFrame\n",
    "    \n",
    "    Outputs:\n",
    "    `p_values_signif`: A Python dictionary in which the keys are the event type\n",
    "    and the values are the significant p-values that resulted from the t-test (alpha\n",
    "    of 0.01)\n",
    "    \n",
    "    Note: If an event type does not have associated events either before or\n",
    "    after the act, ignore it and don't add it to the dictionary (since a t-test\n",
    "    won't be possible)\n",
    "    \"\"\"\n",
    "      \n",
    "    pre_dam = df[df[\"POST_ACT\"]==False][[\"EVENT_TYPE\", \"TOTAL_DAMAGE_DEFLATED\"]].dropna(how=\"any\")\n",
    "    post_dam = df[df[\"POST_ACT\"]==True][[\"EVENT_TYPE\", \"TOTAL_DAMAGE_DEFLATED\"]].dropna(how=\"any\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #dict to add keys produced by ttest of pre_dam and post_dam event types and p-values\n",
    "    p_values_signif = {}\n",
    "    \n",
    "    #variable identifying unique storm event values in df\n",
    "    list_of_unique_events = df['EVENT_TYPE'].unique()\n",
    "    \n",
    "    # creating list of pre/post dam TOTAL_DAMAGE_DEFLATED by event\n",
    "    for i in list_of_unique_events:\n",
    "        pre_dam_unique = pre_dam[pre_dam['EVENT_TYPE']==i].TOTAL_DAMAGE_DEFLATED\n",
    "        post_dam_unique = post_dam[post_dam['EVENT_TYPE']==i].TOTAL_DAMAGE_DEFLATED\n",
    "        \n",
    "        #filtering out events not associated with pre/post dam events to run t-test\n",
    "        if (len(pre_dam_unique)==0) or (len(post_dam_unique)==0):\n",
    "            continue\n",
    "        \n",
    "        #perform t-test of unique events in pre/post dam df\n",
    "        else:    \n",
    "            ttest = pg.ttest(pre_dam_unique, post_dam_unique)\n",
    "            p = ttest['p-val'].item()\n",
    "            alpha =  p <= 0.01\n",
    "            if alpha == True:\n",
    "                p_values_signif[i] = p #adds keys to dict if p-value is not above alpha\n",
    "    \n",
    "    return p_values_signif\n",
    "test_differences(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fafd3",
   "metadata": {},
   "source": [
    "Plotting significant event types and their total deflated damages as box plots, comparing the pre-Act events with the post-Act events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4612177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly box plot - comparing post-act and pre-act \n",
    "#total deflated damages for significant event types.\n",
    "\n",
    "#dumping p-values of significant events to list\n",
    "sig_events = list(test_differences(df).keys())\n",
    "\n",
    "#filtering significant events from df \n",
    "subset_df = df[df.EVENT_TYPE.isin(sig_events)]\n",
    "\n",
    "#df of select columns for plotting and further analysis\n",
    "subset_df = subset_df[['EVENT_TYPE', 'TOTAL_DAMAGE_DEFLATED', 'POST_ACT', 'BEGIN_YEAR']]#.dropna(how='any')\n",
    "\n",
    "#converting BEGIN_YEAR to Int then str to add to map hover text below\n",
    "subset_df['year'] = subset_df['BEGIN_YEAR'].astype('Int64')\n",
    "subset_df['Year'] = subset_df['year'].astype(str)\n",
    "subset_df['Location'] = df['STATE'].astype(str)\n",
    "\n",
    "for event in sig_events:\n",
    "    df_plot = subset_df[subset_df['EVENT_TYPE']==event]\n",
    "\n",
    "    fig = px.box(df_plot,\n",
    "                 x = 'EVENT_TYPE',\n",
    "                 y = 'TOTAL_DAMAGE_DEFLATED',\n",
    "                 color= 'POST_ACT',\n",
    "                 #labels={False: 'pre-Act', True: 'post-Act'},\n",
    "                 template = 'plotly_dark',\n",
    "                 title='<b>Total Deflated Damage of Disaster Type</b><br>post-Act   vs.  pre-Act comparison<br>',\n",
    "                 points='all', #selcts between ‘outliers’, ‘suspectedoutliers’, ‘all’, or False for further analysis\n",
    "                 color_discrete_sequence=['#0000FF', '#DFFF00'], #define CSS-colors\n",
    "                 hover_data = ['Year'],\n",
    "                 hover_name = 'Location', \n",
    "                 category_orders= {'POST_ACT': [False, True]},\n",
    "                )            \n",
    "    \n",
    "    fig.update_layout( # customizes font,legend, orientation & position\n",
    "    font_family='monospace',\n",
    "    legend=dict(\n",
    "        title=None, \n",
    "        orientation='h', \n",
    "        y=1, \n",
    "        yanchor='bottom', \n",
    "        x=0.5, \n",
    "        xanchor='center'),\n",
    "        xaxis_title='Disaster Type',\n",
    "        yaxis_title='Total Damage (Deflated)',\n",
    "        hoverlabel=dict(bgcolor='black',\n",
    "                        font_color = 'white',\n",
    "                        font_size = 12,\n",
    "                       ))\n",
    "    \n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn Plots\n",
    "#dumping p-values of significant events to list\n",
    "sig_events = list(test_differences(df).keys())\n",
    "\n",
    "#filtering significant events from df \n",
    "subset_df = df[df.EVENT_TYPE.isin(sig_events)]\n",
    "\n",
    "#df of select columns for plotting and further analysis\n",
    "subset_df = subset_df[['EVENT_TYPE', 'TOTAL_DAMAGE_DEFLATED', 'POST_ACT', 'BEGIN_YEAR']]#.dropna(how='any')\n",
    "\n",
    "for event in sig_events:\n",
    "    plt.figure(figsize = (15,8))\n",
    "    sns.boxplot(x='EVENT_TYPE', y='TOTAL_DAMAGE_DEFLATED', hue='POST_ACT', data=subset_df[subset_df[\"EVENT_TYPE\"]==event], showfliers=False)\n",
    "    plt.title(\"Pre-Act vs Post-Act Event\" + event)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf71325",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c726fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency table of categorical variable within the significant difference df\n",
    "subset_df_cont = ff.create_table(subset_df.describe(include=['O']).T, index=True, index_title='Categorical columns')\n",
    "subset_df_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a07c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency table of event types with a signifant difference\n",
    "subset_df.EVENT_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of events per type, post-Act and pre-Act\n",
    "subset_df_crosstab = pd.crosstab(index=subset_df['EVENT_TYPE'], columns=subset_df['POST_ACT'])\n",
    "subset_df_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59582a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2 = chi2_contingency(subset_df_crosstab)\n",
    "print('The P-value for event type by post-Act and pre-Act: ', chi2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary of event types and post / pre act status\n",
    "subset_df.groupby('EVENT_TYPE')['POST_ACT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b05cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary of event types and total damages\n",
    "subset_df.groupby('EVENT_TYPE')['TOTAL_DAMAGE_DEFLATED'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c74788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count and statistical summary of post and pre act status by total damages deflated\n",
    "subset_df.groupby('POST_ACT')['TOTAL_DAMAGE_DEFLATED'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b47fef",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd86b65",
   "metadata": {},
   "source": [
    "Scientific studies indicate that extreme weather events such as heatwaves and large storms are likely to become more frequent or more intense with human-induced climate change. The Post-Katrina Emergency Management Reform Act of 2006 sought to remedy the gaps in national emergency recovery operations, which became evident after several devastating natural disasters took place. Out of 62 unique disaster types, nine events returned P-values less than 0.00. Based on the hypothesis tests results, we can conclude a statistically high significance between nine disaster types and the implementation of PKEMRA. <br><br>As we continue to slice the data for deeper analysis, we uncover several notions. For instance, flash floods have the highest frequency out of the nine disaster types, followed by tornados, heavy snow, and droughts. When analyzing the total damages for these disasters, flash floods, tornados, and droughts are the costliest weather and climate disasters. Though heavy snow may not have as high a total damage cost, it is the fourth most frequent event among the nine types. Natural disasters have cost our nation trillions of dollars, and in efforts to mitigate the risk and loss of such disasters, PKEMRA was enacted in 2006. An interesting question to consider is how PKEMRA has impacted the total damages of storms. <br><br>Natural disasters can be predicted at different levels as frequency is derived either from the number of recorded events or by developing models of events exampled in this case. The box plot visualized in 8.2 does not yet give us a complete story. Although there is high statistical significance in nine of 62 disaster types, there are still many other risk factors to consider. Nonetheless, governments must bring about awareness, preparedness, and warning systems to reduce the impact of natural disasters on communities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3623151",
   "metadata": {},
   "source": [
    "By Veronica Huxley"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
